# ðŸ¤– LLM Model Setup Guide

Configure the local LLM for test report summarization.

---

## ðŸ“‹ Quick Start

```bash
# 1. Download the model
curl -L -o ~/Models/Qwen2.5-0.5B-Instruct-Q4_0.gguf \
  "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_0.gguf"

# 2. Set environment variable
export LLM_MODEL_PATH=~/Models/Qwen2.5-0.5B-Instruct-Q4_0.gguf

# 3. Run summarizer with LLM
cd e2e/schemathesis && source venv/bin/activate
python ../scripts/summarize_playwright_results.py
```

---

## ðŸ”§ Configuration

### Environment Variable

The summarizers use `LLM_MODEL_PATH` to locate the model:

```bash
# Option 1: Export in shell
export LLM_MODEL_PATH="/path/to/your/model.gguf"

# Option 2: Add to .bashrc / .zshrc
echo 'export LLM_MODEL_PATH="$HOME/Models/Qwen2.5-0.5B-Instruct-Q4_0.gguf"' >> ~/.zshrc

# Option 3: Set per-command
LLM_MODEL_PATH=/path/to/model.gguf python scripts/summarize_playwright_results.py
```

### Default Path

If `LLM_MODEL_PATH` is not set, the scripts fall back to:
```
/Users/lucian.cernauteanuthinslices.com/Documents/LLM Models/Qwen2.5-0.5B-Instruct-Q4_0.gguf
```

---

## ðŸ“¥ Model Download Options

### Option 1: Direct Download (Recommended)

```bash
# Create models directory
mkdir -p ~/Models

# Download Qwen 2.5 0.5B (Q4 quantized, ~400MB)
curl -L -o ~/Models/Qwen2.5-0.5B-Instruct-Q4_0.gguf \
  "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_0.gguf"
```

### Option 2: Hugging Face CLI

```bash
# Install huggingface_hub
pip install huggingface_hub

# Download
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct-GGUF \
  qwen2.5-0.5b-instruct-q4_0.gguf \
  --local-dir ~/Models
```

### Option 3: Git LFS

```bash
# Clone the repo (large download)
git lfs install
git clone https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF ~/Models/Qwen-GGUF
```

---

## ðŸ“Š Model Comparison

| Model | Size | Speed | Quality | RAM |
|-------|------|-------|---------|-----|
| **Qwen2.5-0.5B-Q4** | ~400MB | âš¡ Fast | Good | ~1GB |
| Qwen2.5-1.5B-Q4 | ~1GB | Medium | Better | ~2GB |
| Qwen2.5-3B-Q4 | ~2GB | Slower | Best | ~4GB |

**Recommendation:** Start with 0.5B for speed. Upgrade if recommendations feel shallow.

---

## âš ï¸ LLM Limitations

The 0.5B model is small and may:

- Generate shallow or generic recommendations
- Occasionally repeat phrases
- Miss subtle patterns in test data
- Produce slightly incorrect technical advice

**Always verify AI insights against actual test results.**

The summarizers include a disclaimer when LLM output is shown:

> âš ï¸ **Disclaimer:** AI insights are generated by a small 0.5B parameter model and may occasionally be shallow or imprecise.

---

## ðŸ Python Dependencies

The LLM requires `llama-cpp-python`:

```bash
# Using the existing venv
cd e2e/schemathesis
source venv/bin/activate

# Install (already in requirements.txt)
pip install llama-cpp-python

# Verify
python -c "from llama_cpp import Llama; print('OK')"
```

### Troubleshooting Installation

```bash
# If llama-cpp-python fails to build:

# macOS with Apple Silicon
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall

# Linux with CUDA
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall

# CPU only (slower but always works)
pip install llama-cpp-python --force-reinstall
```

---

## ðŸ§ª Test the Setup

```bash
# Quick test
cd e2e/schemathesis && source venv/bin/activate

# Test LLM directly
python -c "
from llama_cpp import Llama
import os

model_path = os.environ.get('LLM_MODEL_PATH', 'path/to/model.gguf')
print(f'Loading model from: {model_path}')

llm = Llama(model_path=model_path, n_ctx=512, verbose=False)
output = llm('Q: What is 2+2? A:', max_tokens=10)
print('Response:', output['choices'][0]['text'])
"

# Test summarizer with LLM
python ../scripts/summarize_playwright_results.py --results ../allure-results
```

---

## ðŸ”’ CI/CD Notes

In GitHub Actions, LLM is **disabled by default** (`--ci` mode) because:

1. Model file is large (~400MB) and slow to download
2. CI should produce deterministic results
3. LLM adds ~10-30 seconds to report generation

To enable LLM in CI (not recommended):

```yaml
- name: Download LLM Model
  run: |
    curl -L -o model.gguf "https://huggingface.co/.../model.gguf"
    echo "LLM_MODEL_PATH=$(pwd)/model.gguf" >> $GITHUB_ENV

- name: Generate Summary with LLM
  run: python scripts/summarize_playwright_results.py  # Remove --ci flag
```

---

## ðŸ“š Related Documentation

- **[LLM_SUMMARIZER_GUIDE.md](./LLM_SUMMARIZER_GUIDE.md)** - Full summarizer documentation
- **[PLAYWRIGHT_SUMMARIZER_GUIDE.md](./PLAYWRIGHT_SUMMARIZER_GUIDE.md)** - Playwright-specific guide
- **[TESTING_STRATEGY.md](../TESTING_STRATEGY.md)** - Overall testing approach
